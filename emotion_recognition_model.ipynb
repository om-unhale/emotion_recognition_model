{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Important library"
      ],
      "metadata": {
        "id": "HGVOTWJ9Idhy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9xT5rTW-rhm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load validation and training dataset\n",
        "\n",
        "dataset available at : https://www.kaggle.com/datasets/msambare/fer2013"
      ],
      "metadata": {
        "id": "htFgRQg-Ilnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess the FER2013 dataset or any we want to use\n",
        "# Assume dataset is in 'data/train' and 'data/validation' in folders\n",
        "# dataset available at : https://www.kaggle.com/datasets/msambare/fer2013\n",
        "batch_size = 32\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    'data/train',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    'data/validation',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "TQp8JhMjNPK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model training"
      ],
      "metadata": {
        "id": "k8bih3PgI_J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))# Step 2: Load the pretrained ResNet50 model\n",
        "\n",
        "for layer in base_model.layers:# Freeze the base model layers\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "output = Dense(train_data.num_classes, activation='softmax')(x) # Add custom layers\n",
        "\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output) # Combine base and custom layers\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),        #Compile the model\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VOymvs_9WEHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "CxWp4pY1JdDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),   # Recompile with a lower learning rate\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 5\n",
        "fine_tune = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=fine_tune_epochs\n",
        ")                              # Fine-tune the model\n",
        "\n"
      ],
      "metadata": {
        "id": "wESZ2CMce6As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparision variouse matrices"
      ],
      "metadata": {
        "id": "cK_cYi1PKIs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune_epochs = 5\n",
        "fine_tune = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=fine_tune_epochs\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('emotion_recognition_model.h5')\n",
        "\n",
        "# Print final accuracy on validation set\n",
        "val_accuracy = fine_tune.history['val_accuracy'][-1]\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fine_tune.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(fine_tune.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Epoch vs. Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(fine_tune.history['loss'], label='Training Loss')\n",
        "plt.plot(fine_tune.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Epoch vs. Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6wYAzmh0gHJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}